# k-近邻学习读书报告

## 读数摘要：

### k-近邻学习

**概述：**惰性学习，不对训练样本做任何操作，而是通过将测试例与所有训练样例比较选出最近似的k个样本，再由k近邻中出现最多的类别决定。

**代码：**

~~~~python
def KNN(D,C,d,k):
    knb=[]
    for i in range(len(D)):#遍历一遍训练集
        d = distance(D[i],d)#事先定义好的距离函数
		for j in range(k):#插入排序选出最近的k个训练例
            if knb[j]==None:
                knb[j]=(i,d)
                break
            elif d<knb[j][1]:
                if j==k:
                    knb[k]=(i,d)
            	else:
                    knb.insert(j,(i,d))
            	break
    kcc={}
    for nb in knb:#遍历一遍k近邻统计各类别频次
        if kcc[C[nb[0]]]==None:
            kcc[C[nb[0]]]==1
        else:
            kcc[C[nb[0]]]+=1
    return max(kcc)#返回具有最大频次的类别
~~~~

### 分类器的集成

#### Bagging

**概述：**通过随机抽取由原始训练集生成k个训练集，再由这k个训练集投标决定分类。

**特点：**提高稳定性，降低敏感度。

#### Boosting

**概述：**训练多个弱学习器，直到完美预测训练集或添加最大数量的模型。每轮学习的样本权重分配不同，原则是为了使得不同的学习器通过不同的学习模式来弥补其他分类器的不足，上一轮中被错误分类较多的训练样例将被加大权重，正确分类较多的训练样例将被减少权重。在组合最终的强学习器时，通过各学习器在各自测试过程中的表现决定其决策权重。

**缺点：**对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。

**代码样例（AdaBoost）：**

~~~~

~~~~

## 交流讨论

**问题1（来自潘欣）：**什么是余弦相似度？

**我的回答：**通过计算两个向量的夹角余弦值来评估他们的相似度。

**问题2（来自丁易）：**bagging什么情况下会降低稳定的分类器的准确率？

**我的回答：**例如，某分类器具有某一类别，此类别仅具有少量训练样例，那么bagging算法使得集成分类器对于这一弱势类别更加缺乏敏感性。

**问题3（来自李嘉逸）：**knn在样本不平衡的时候会有缺陷，例如有一类样本容量很大，其他类样本容量小，导致大容量的样本在k个邻居中占多数，有无改进的办法。

**我的回答：**通过增加弱势类别样例的权重

**问题4（来自李嘉逸）：**集成学习选取多个学习器然后综合评价为什么可以获得更好的分类结果，换句话说，分类器中必定有好的有坏的，最终结果从生活经验来看不是应该趋向于最终的平均值吗？

**我的回答：**我的理解是，首先通过赋予不同权重，来使得不同弱分类器能偏向于关注某一容易被其他弱分类器错分的类，再通过给不同分类器根据正确率分配不同的投票权重，来减少其对噪声的敏感性。两层分权的最终结果是，容易将噪声与弱势类别区分出来，提高准确率。