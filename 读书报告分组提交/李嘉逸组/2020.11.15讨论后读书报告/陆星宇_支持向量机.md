本次阅读范围：《web数据挖掘》3.8

### 读书摘要

#### 支持向量机（SVM)

设训练集合$D$为{$(x_1,y_1),...,(x_n,y_n)$}，其中

$x_i$$=(x_{i1},...x_{ir})$是一个r维输入向量，$y_i\in${-1,1}是类标记。

线性函数$f(x)=<w\cdot x>+b$作为分类器，值为正则类标赋1，反之-1。

本质上是在寻找一个超平面$<w\cdot x>+b=0$，称为决策边界。

两个待解决问题：

1. 对于线性可分的问题，如何选择合适的决策界面？
2. 对于非线性可分的问题如何处理？

#### 可分情况

首先注意$<w\cdot x>+b=0$与$<\lambda w\cdot x>+\lambda b$之间的等价性；

支持向量机要最大化正例与负例之间的边距margin=$d_++d_-$，其中$d_+$，$d_-$分别为决策边界到正例与负例的最小距离。（原因参考计算学习理论中结构风险最小化结论）

考察经过最接近决策边界的正负例并与决策边界平行的两个超平面：
$$
H_+:<w\cdot x>+b=1
$$

$$
H_-:<w\cdot x>+b=-1
$$



使得
$$
H_+:<w\cdot x_i>+b\ge1，if~y_i=1
$$

$$
H_-:<w\cdot x_i>+b\le-1,if~y_i=-1
$$

记$x_s$为决策边界上点，则\
$$
margin=d_++d_-=\frac{|<w\cdot x_s>+b-1|}{||w||}+\frac{|<w\cdot x_s>+b+1|}{||w||}=\frac{2}{||w||}
$$
如此，原问题转化为凸优化问题：
$$
minimize:\frac{<w\cdot w>}{2}
$$

$$
subject~~to:1-y_i(<w\cdot x_i>+b)\le0
$$

/*

可利用**标准拉格朗日乘子方法**（?）解决（具体理论参考《凸优化》）：
$$
L_p=\frac{1}{2}<w\cdot w>+\sum_{i=1}^{n}\alpha_i[1-y_i(<w\cdot x_i>+b)]~~(\alpha_i\ge0)
$$
$\alpha-i>0$对应的向量称为支持向量;

最优解条件（Kuhn-Tucker条件）：
$$
\frac{\delta L_p}{\delta w_j}=w_j-\sum_{i=1}^{n}y_i\alpha_ix_{ij}=0,j=1,...,r
$$

$$
\frac{\delta L_p}{\delta b}=-\sum_{i=1}^{n}y_i\alpha_i=0
$$

$$
1-y_i(<w\cdot x_i>+b)\le0,i=1,...,n
$$

$$
\alpha_i\ge0,i=1,...,n
$$

$$
\alpha_i[1-y_i(<w\cdot x_i>+b)]=0,i=1,...,n
$$

*/

代回主问题将其转化为**对偶问题**（?）(Wolfe Dual)：
$$
Maxmize:L_D=\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i,j=1}^{n}y_iy_j\alpha_i\alpha_j<x_i\cdot x_j>
$$

$$
subject~to:\sum_{i=1}^{n}y_i\alpha_i=0,\alpha_i\ge0,i=1,2,...n
$$

Wolfe Dual有如下性质，使得$L_D$最大的$\alpha_i$可以推出使得$L_p$最小时的$w$和$b$；考虑到数值解的误差，$b$由支持向量计算最后取均值得到。

最终的决策边界是：
$$
<w\cdot x>+b=\sum_{i\in sv}y_i\alpha_i<x_i\cdot x_i>+b=0
$$

#### 数据不可分（线性支持向量机）

现实情况：训练数据的噪声与实际问题的随机性使得数据非线性可分。

引入松弛变量$\xi_i\ge0$将约束转化为：
$$
y_i(<w\cdot x_i>+b)\ge 1-\xi_i,~~~~i=1,...,n
$$
引入误差代价，将目标函数转化为：
$$
minimize:\frac{<w\cdot w>}{2}+C(\sum_{i=1}^{n}\xi_i)^k
$$
其中$C\ge0$，$k$常取1；

新的优化问题(软边距支持向量机)是：
$$
minimize:\frac{<w\cdot w>}{2}+C\sum_{i=1}^n\xi_i
$$

$$
subject~to:y_i(<w\cdot x_i>+b)\ge1-\xi_i,~~~i=1,2,...,n
$$

$$
\xi_i\ge0,~~~i=1,2,...,n
$$

设$\xi_i$的拉格朗日乘子为$\mu_i$，推导过程太长了故略，得到对偶问题与线性可分情况相同，唯一区别是$\alpha_i\le C$，由数值方法求解得到$\alpha_i$，可以用来求解$w$和$b$：
$$
w_j=\sum_{i=1}^ny_i\alpha_ix_{ij}
$$

$$
b_i=\frac{1}{y_i}-\sum_{i=1}^ny_i\alpha_i<x_i\cdot x_i>
$$

//这里书上有错吗？

PS:$\alpha与\xi的关系$...

#### 数据不可分（核方法）

中心思想：将线性不可分的输入向量变换到另一个可以线性可分的空间。原始数据空间称为**输入空间**，新的空间称为**特征空间**，

可能问题：维数灾难；解决方法：直接计算$<\phi(x)\cdot\phi(z)>$以避免直接计算$\phi(x)$.

记$K(x,z)=<\phi(x)\cdot\phi(z)>$为核函数，一个例子是多项式核
$$
K(x,z)=(z\cdot x)^d
$$
,证明参考Mercer定理。

另一个常用的高斯径向基函数（GaussianRBF）：
$$
K(x,z)=e^{-||x-z||^2/2\sigma}
$$
其中，$\theta\in R,d\in N,\sigma＞0$.

### 总结

**支持向量机的优点：**

1. 利用最大边距决策分割正例与反例；
2. 核函数解决非线性可分问题与空间变换造成的维数灾难。

**支持向量机局限：**

1. 无法直接处理离散型属性；
2. 只允许二类分类；
3. 产生的超平面常常由于其高维无法被人类认知。

**阅读过程存在的问题：**

1. 计算学习理论中结构风险最小化结论；
2. 对凸优化问题的拉格朗日处理；
3. 对偶问题；
4. Mercer定理；
5. 常用核函数及其原理以及应用场景。
6. 以上问题需要额外自学相关理论，小组交流内容局限，故不展开讨论。

### 理论补充

#### 序列最小优化算法



### 代码实现

带核函数参数与噪声处理的支持向量机

~~~~python
##代码我还没写完，写完后我把更新后的读书笔记上传github
~~~~





