# 部分监督学习

## 5.1从已标注数据和未标注数据中学习

### 5.1.1使用朴素贝叶斯分类器的EM算法

EM算法由两个步骤组成：

Expectation Step & Maximization Step.

1. 利用已标注数据集L构造分类器，给未标注数据集U计算概率分布；
2. 利用所有已标注数据和已计算概率分布数据构造新分类器；
3. 循环直到分类器参数不再变化或变化极小。

~~~~python
def EM(L,U,maxgen，endcon):#参数分别为（已标注数据集，未标注数据集，最大迭代次数，终止条件）
    clsf = NBlearn(L);#由NB算法通过已标注数据集学习一个初始分类器
    gen = 1#迭代次数计数
    while 1:
        gen += 1
        #E-step
        for i in range(len(U)):
            Pr[i] = clsf.NB()#由分类器给每个未标注数据集分配一个新概率分布
        #M-step
        nclsf = NBlearn(L,U,Pr)#由新的概率分布和已标注数据集学习一个新分类器
        if endneed(endcon,clsf,nclsf) or gen > maxgen:
            break
        else:
            clsf = nclsf
    return clsf#返回最终的分类器
~~~~

在满足以下假设时，算法具有较好的效果：

1. 数据是由某个混合模型生成的；
2. 在混合成员与分类类别之间有着一一对应关系；

第二个假设实际上较难满足，分类器性能可能随着循环次数的增加而降低。可以用两种方法来解决这个问题：

1. 给无标注数据加权；
2. 寻找混合分量

### 5.1.2 Co-Training

这种方法假设数据集中的属性可以被分为两个集合，其中每个都足够学习得到目标分类函数，这使得我们可以利用相同训练数据的两个特征集合学习得到两个分类器。两个分类器应该有相同的结果。

在某些进一步的假设下，在使用一个从很少量已标注数据中学来的很弱的分类器情况下，Co-training通过利用属性的分割来从已标注和无标注数据中来进行迭代学习：

1. 在不同目标函数下数据的类别分布是相容的，即预测结果相同；
2. 两个特征之间条件独立，这在实践中似乎是个不太实际的假设。

其主要思想是，在每次循环中，它首先从已标注数据集L的每个特征集中学习得到一个分类器，然后再利用这个分类器去分类未标注数据集U中的无标注数据，将U中标为类$C_i$的一定数量，分类可信度高的数据加入到L中。这个过程一直进行到U成为空集（或者达到一定的分类次数）。

### 5.1.3 自学习

一种不需要特征分割的增量式算法。

### 5.1.4 直推式支持向量机(Transduction SVM)

在训练SVM过程中使用无标注数据的一个方法是，为无标注数据选择类别标识使得得到的分类器具有最大的间隙。

以标注那些已知的测试(无标注)数据为目的的训练过程被称为是直推式的。







